{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "import math\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import sys\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import albumentations.pytorch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import csv \n",
    "import timm\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "batch_size = 16\n",
    "class_n = 2\n",
    "learning_rate = 1e-5 # 1e-4\n",
    "epochs = 10\n",
    "save_path = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3b1921z1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 80900... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4873059b546141db875d51ede497e95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">NB1D_final_data</strong>: <a href=\"https://wandb.ai/boostcampcv08/NPDH/runs/3b1921z1\" target=\"_blank\">https://wandb.ai/boostcampcv08/NPDH/runs/3b1921z1</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211118_072847-3b1921z1/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3b1921z1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/boostcampcv08/NPDH/runs/2skgihwq\" target=\"_blank\">NB1D_final_data</a></strong> to <a href=\"https://wandb.ai/boostcampcv08/NPDH\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/boostcampcv08/NPDH/runs/2skgihwq?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f43a2e343a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'code.ipynb'\n",
    "wandb.init(project='NPDH', name='NB1D_final_data',\n",
    "    config={\n",
    "    \"batch size\": batch_size,\n",
    "    \"epochs\" : epochs,\n",
    "    \"learning rate\": learning_rate,\n",
    "    # \"backborn\" : 'se_resnext101_32x4d', \n",
    "    # \"architecture\" : 'Deeplabv3+', \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(ground_truth, pred_output, labels = [1,0]):\n",
    "    mtris = sklearn.metrics.confusion_matrix(ground_truth,pred_output, labels =[1,0])\n",
    "    TP, FN = mtris[0]\n",
    "    FP, TN = mtris[1]\n",
    "    ttl = TP+FN+FP+TN\n",
    "    accuracy = (TP+TN)/ttl\n",
    "    specificity = TN/(TN+FP)\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    precision = TP/(TP+FP)\n",
    "    negative_predicable_value = TN/(TN+FN)\n",
    "    F1score = 2*precision*sensitivity/(precision+sensitivity)\n",
    "\n",
    "    return accuracy,specificity,sensitivity,precision,negative_predicable_value,F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = os.path.join(os.getcwd(),'a-trac-colon')\n",
    "# colon_negative = 'colon_negative'\n",
    "# colon_negative_test = 'colon_negative_test'\n",
    "# colon_positive = 'colon_positive'\n",
    "# colon_positive_test = 'colon_positive_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_negative_list = os.listdir(os.path.join(base_path,colon_negative))\n",
    "# test_negative_list = os.listdir(os.path.join(base_path,colon_negative_test))\n",
    "# train_positive_list = os.listdir(os.path.join(base_path,colon_positive))\n",
    "# test_positive_list = os.listdir(os.path.join(base_path,colon_positive_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_nagative = [(img_name, os.path.join(base_path,colon_negative,img_name),0) for img_name in train_negative_list]\n",
    "# train_positive = [(img_name, os.path.join(base_path,colon_positive,img_name),1) for img_name in train_positive_list]\n",
    "# train_pd = pd.DataFrame(train_nagative+train_positive, columns = ['image_id','image_path','labels'])\n",
    "# train_pd.to_csv('train_pd.csv')\n",
    "# train_pd = train_pd.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_nagative = [(img_name, os.path.join(base_path,colon_negative,img_name),0) for img_name in test_negative_list]\n",
    "# test_positive = [(img_name, os.path.join(base_path,colon_positive,img_name),1) for img_name in test_positive_list]\n",
    "# test_pd = pd.DataFrame(test_nagative+test_positive, columns = ['image_id','image_path','labels'])\n",
    "# test_pd.to_csv('test_pd.csv')\n",
    "# test_pd = test_pd.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = pd.read_csv(\"train_1_5.csv\")\n",
    "test = pd.read_csv(\"test_1_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Image_1.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Image_100.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Image_1000.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Image_1001.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Image_1002.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6395</th>\n",
       "      <td>6395</td>\n",
       "      <td>Image_992.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>6396</td>\n",
       "      <td>Image_994.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6397</th>\n",
       "      <td>6397</td>\n",
       "      <td>Image_995.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6398</th>\n",
       "      <td>6398</td>\n",
       "      <td>Image_997.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6399</th>\n",
       "      <td>6399</td>\n",
       "      <td>Image_998.jpg</td>\n",
       "      <td>/opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        image_id  \\\n",
       "0              0     Image_1.jpg   \n",
       "1              1   Image_100.jpg   \n",
       "2              2  Image_1000.jpg   \n",
       "3              3  Image_1001.jpg   \n",
       "4              4  Image_1002.jpg   \n",
       "...          ...             ...   \n",
       "6395        6395   Image_992.jpg   \n",
       "6396        6396   Image_994.jpg   \n",
       "6397        6397   Image_995.jpg   \n",
       "6398        6398   Image_997.jpg   \n",
       "6399        6399   Image_998.jpg   \n",
       "\n",
       "                                             image_path  labels  \n",
       "0     /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...       0  \n",
       "1     /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...       0  \n",
       "2     /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...       0  \n",
       "3     /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...       0  \n",
       "4     /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_negati...       0  \n",
       "...                                                 ...     ...  \n",
       "6395  /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...       1  \n",
       "6396  /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...       1  \n",
       "6397  /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...       1  \n",
       "6398  /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...       1  \n",
       "6399  /opt/ml/nunbody/NHDP/a-trac-gi/final/gi_positi...       1  \n",
       "\n",
       "[6400 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread(train_total['image_path'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(train_total['image_path'][0])\n",
    "img = np.transpose(img, (2,0,1))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, files, labels=None, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.files = files\n",
    "        if mode == 'train':\n",
    "            self.labels = labels\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if self.mode == 'train':\n",
    "            img = cv2.imread(self.files[i])\n",
    "            # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "            img = img.astype(np.float32)/255\n",
    "            img = np.transpose(img, (2,0,1))\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.labels[i], dtype=torch.long),\n",
    "                'img_id' : self.files[i].split('/')[-1]\n",
    "            }\n",
    "        else:\n",
    "            img = cv2.imread(self.files[i])\n",
    "            # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "            img = img.astype(np.float32)/255\n",
    "            img = np.transpose(img, (2,0,1))\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'img_id' : self.files[i].split('/')[-1]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_total\n",
    "val = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train['image_path'], train['labels'].values)\n",
    "val_dataset = CustomDataset(val['image_path'], val['labels'].values)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "test_dataset = CustomDataset(test['image_path'], labels=None, mode='test')\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[0.9922, 0.9922, 0.9922,  ..., 0.8745, 0.7961, 0.7529],\n",
       "          [0.9922, 0.9882, 0.9882,  ..., 0.8667, 0.8314, 0.7059],\n",
       "          [0.9922, 0.9961, 0.9922,  ..., 0.9333, 0.8588, 0.6667],\n",
       "          ...,\n",
       "          [0.8235, 0.7843, 0.6863,  ..., 0.9725, 0.9490, 0.9373],\n",
       "          [0.6706, 0.7490, 0.8275,  ..., 0.9569, 0.9490, 0.9373],\n",
       "          [0.5765, 0.5922, 0.6667,  ..., 0.9412, 0.9255, 0.9059]],\n",
       " \n",
       "         [[0.9922, 0.9922, 0.9922,  ..., 0.7294, 0.6235, 0.4667],\n",
       "          [0.9922, 0.9882, 0.9882,  ..., 0.7529, 0.6941, 0.4549],\n",
       "          [0.9922, 0.9961, 0.9922,  ..., 0.8392, 0.7098, 0.4039],\n",
       "          ...,\n",
       "          [0.6353, 0.6039, 0.4824,  ..., 0.9137, 0.8745, 0.8549],\n",
       "          [0.4275, 0.5373, 0.6588,  ..., 0.9059, 0.8784, 0.8549],\n",
       "          [0.1804, 0.2353, 0.4157,  ..., 0.8902, 0.8667, 0.8353]],\n",
       " \n",
       "         [[0.9922, 0.9922, 0.9922,  ..., 0.8510, 0.7451, 0.5882],\n",
       "          [0.9922, 0.9882, 0.9922,  ..., 0.8627, 0.8118, 0.5765],\n",
       "          [0.9922, 0.9961, 0.9961,  ..., 0.9333, 0.8235, 0.5216],\n",
       "          ...,\n",
       "          [0.7608, 0.6980, 0.5529,  ..., 0.9882, 0.9647, 0.9569],\n",
       "          [0.4745, 0.5686, 0.6941,  ..., 0.9882, 0.9804, 0.9725],\n",
       "          [0.2039, 0.2549, 0.4431,  ..., 0.9804, 0.9725, 0.9569]]]),\n",
       " 'label': tensor(0),\n",
       " 'img_id': 'Image_1.jpg'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownsamplerBlock(nn.Module):\n",
    "    def __init__(self, in_channels=32, out_channels=32):\n",
    "        super(DownsamplerBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels-in_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = torch.cat([self.conv(input_tensor), self.maxpool(input_tensor)], dim=1)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "class NB1D(nn.Module):\n",
    "    def __init__(self, channels=32, dilation=1):\n",
    "        super(NB1D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=(3, 1), stride=1,\n",
    "                               padding=(1, 0))\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=(1, 3), stride=1,\n",
    "                               padding=(0, 1))\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=(3, 1), stride=1,\n",
    "                               padding=(1*dilation, 0), dilation=dilation)\n",
    "        self.conv4 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=(1, 3), stride=1,\n",
    "                               padding=(0, 1*dilation), dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn2(x)\n",
    "        return F.relu(x+input_tensor)\n",
    "\n",
    "class ResNet50NB1D(nn.Module):\n",
    "    def __init__(self, color_channel=3, num_classes=2):\n",
    "        super(ResNet50NB1D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=color_channel, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv2_x = nn.Sequential(\n",
    "            NB1D(channels=64),\n",
    "            NB1D(channels=64),\n",
    "            # NB1D(channels=64)\n",
    "        )\n",
    "        self.conv3_x = nn.Sequential(\n",
    "            DownsamplerBlock(in_channels=64, out_channels=128),\n",
    "            NB1D(channels=128),\n",
    "            NB1D(channels=128),\n",
    "            # NB1D(channels=128),\n",
    "            # NB1D(channels=128)\n",
    "        )\n",
    "        self.conv4_x = nn.Sequential(\n",
    "            DownsamplerBlock(in_channels=128, out_channels=256),\n",
    "            NB1D(channels=256),\n",
    "            NB1D(channels=256),\n",
    "            # NB1D(channels=256),\n",
    "            # NB1D(channels=256),\n",
    "            # NB1D(channels=256),\n",
    "            # NB1D(channels=256)\n",
    "        )\n",
    "        self.conv5_x = nn.Sequential(\n",
    "            DownsamplerBlock(in_channels=256, out_channels=512),\n",
    "            NB1D(channels=512),\n",
    "            NB1D(channels=512),\n",
    "            # NB1D(channels=512),\n",
    "        )\n",
    "        \n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4_x(x)\n",
    "        # x = self.conv5_x(x)\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "model = ResNet50NB1D().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /opt/ml/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250a2549b612445fa7007ecf27208195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/254M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b7\n",
      "네트워크 필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 (예측 class type 개수) 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b7',num_classes=2)\n",
    "print(\"네트워크 필요 입력 채널 개수\", model._conv_stem.weight.shape[1])\n",
    "print(\"네트워크 출력 채널 개수 (예측 class type 개수)\", model._fc.weight.shape[0])\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(ground_truth, pred_output, labels = [1,0]):\n",
    "    mtris = sklearn.metrics.confusion_matrix(ground_truth,pred_output, labels=labels)\n",
    "    TP, FN = mtris[0]\n",
    "    FP, TN = mtris[1]\n",
    "    ttl = TP+FN+FP+TN\n",
    "    accuracy = (TP+TN)/ttl\n",
    "    specificity = TN/(TN+FP)\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    precision = TP/(TP+FP)\n",
    "    negative_predicable_value = TN/(TN+FN)\n",
    "    F1score = 2*precision*sensitivity/(precision+sensitivity)\n",
    "\n",
    "    return accuracy,specificity,sensitivity,precision,negative_predicable_value,F1score\n",
    "\n",
    "def train_step(batch_item, epoch, batch, training):\n",
    "    img = batch_item['img'].to(device)\n",
    "    label = batch_item['label'].to(device)\n",
    "    img_id = batch_item['img_id']\n",
    "    ground_truth = []\n",
    "    pred_output = []\n",
    "    if training is True:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "            pred_output+=output.argmax(axis =1).cpu().tolist()\n",
    "            ground_truth+=label.cpu().tolist()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss,ground_truth,pred_output, img_id\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "            pred_output+=output.argmax(axis =1).cpu().tolist()\n",
    "            ground_truth+=label.cpu().tolist()\n",
    "        return loss,ground_truth,pred_output, img_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'save_model/NB1D_final_data.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5685c4e03c364198bd570bf6203f2dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d903e074c534125b284ec0072149185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60ea189b7b4477d9d5e084cb0e8769a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d413d837212b425a88b896381b64d218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfac34677474c24b07df46f1a0d9be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d006fbe46354b7d938012c0104a8581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dd90dd1b224ba49ade3c4a2ea3f1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0981ec07544448bd6c696bb46d5c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e63a8ca64db44f0a164bd92b3f51a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f2d9a03c16472582e72db93a165763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55727ed8c93749d1aeb131310e1f0118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a068c4aea084c1181a14f8212ba22fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6540b8671ac040579ba5191ea03a4865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b53309954a4b9ebd29c42c48f66868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b3e5e558234250b43ab28959e74876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17cb4c6f8c647c49a5018c729d871ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd41ae0f969d42cab003c7c6795c7236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e492573a0979431e8c4459fdcd94d2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d306f409084bd3961b5ac1dd633824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8012586049924bd0b47e322adb023d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "    training = True\n",
    "    ground_truth = []\n",
    "    pred_output = []\n",
    "    \n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss,gt,po,_ = train_step(batch_item, epoch, batch, training)\n",
    "        total_loss += batch_loss\n",
    "        ground_truth += gt\n",
    "        pred_output += po\n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1))\n",
    "        })\n",
    "    loss_plot.append(total_loss/(batch+1))\n",
    "    acc,spec,sens,prec,npv,f1 = evaluation(ground_truth,pred_output)\n",
    "\n",
    "    wandb.log({\n",
    "            'train Total Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
    "            \"train Accuracy\" : acc, \n",
    "            \"train Specificity\" : spec,\n",
    "            \"train Sensitivity\" : sens,\n",
    "            \"train Precision\" : prec,\n",
    "            \"train Negative_Predicable_Value\" : npv,\n",
    "            \"train F1score\" : f1,\n",
    "            \"train total_mean\" : (acc+spec+sens+prec+npv+f1)/6\n",
    "        })\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "    training = False\n",
    "    ground_truth = []\n",
    "    pred_output = []   \n",
    "    image_ids = []\n",
    "    \n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss,gt,po,ids = train_step(batch_item, epoch, batch, training)\n",
    "        total_val_loss += batch_loss\n",
    "        ground_truth+=gt\n",
    "        pred_output+=po    \n",
    "        image_ids+=ids\n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1))\n",
    "        })\n",
    "        \n",
    "    acc,spec,sens,prec,npv,f1 = evaluation(ground_truth,pred_output)\n",
    "    wandb.log({\n",
    "            \"val Total Loss\": '{:06f}'.format(total_val_loss/(batch+1)),\n",
    "            \"val Accuracy\" : acc, \n",
    "            \"val Specificity\" : spec,\n",
    "            \"val Sensitivity\" : sens,\n",
    "            \"val Precision\" : prec,\n",
    "            \"val Negative_Predicable_Value\" : npv,\n",
    "            \"val F1score\" : f1,\n",
    "            \"val total_mean\" : (acc+spec+sens+prec+npv+f1)/6\n",
    "    })\n",
    "    val_loss_plot.append(total_val_loss/(batch+1))\n",
    "    \n",
    "    if min(val_loss_plot) == val_loss_plot[-1]:\n",
    "        torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", 'w') as file: \n",
    "    writer = csv.writer(file) \n",
    "    writer.writerow(pred_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_output),len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(ground_truth,pred_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test['image_path'], labels=None, mode='test')\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, num_workers=0, shuffle=False)\n",
    "\n",
    "# test_inference\n",
    "tqdm_test_dataset = tqdm(enumerate(test_dataloader))\n",
    "training = True\n",
    "pred_output = []\n",
    "ground_truth = []\n",
    "for batch, batch_item in tqdm_test_dataset:\n",
    "    model.eval()\n",
    "    img = batch_item['img'].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        pred_output.append(int(output.argmax().cpu()))\n",
    "        ground_truth.append(val.labels.iloc[batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.argmax(axis =1).cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output = [int(i.cpu())  for i in pred_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mtris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5159/1325227633.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mFP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mtris' is not defined"
     ]
    }
   ],
   "source": [
    "TP, FN = mtris[0] \n",
    "FP, TN = mtris[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1000)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [1,0,1,0,0,0]\n",
    "p = [1,1,0,0,0,0]\n",
    "sklearn.metrics.confusion_matrix(t,p, labels =[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "inbody",
   "language": "python",
   "name": "inbody"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
